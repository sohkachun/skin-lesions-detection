{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8cedbf0-8421-4edb-88d8-d6c40799ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a5ec7-67a5-4941-8a2a-f3700b816597",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe0a4df-c15a-4d3e-9c71-1e094eb5bce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_5622764</td>\n",
       "      <td>65.0</td>\n",
       "      <td>male</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>7.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_8296521</td>\n",
       "      <td>50.0</td>\n",
       "      <td>male</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_1262887</td>\n",
       "      <td>65.0</td>\n",
       "      <td>male</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>6.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_9555133</td>\n",
       "      <td>55.0</td>\n",
       "      <td>male</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>9.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_2851454</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  age_approx   sex anatom_site_general  clin_size_long_diam_mm  \\\n",
       "0  ISIC_5622764        65.0  male     posterior torso                    7.79   \n",
       "1  ISIC_8296521        50.0  male     posterior torso                    6.90   \n",
       "2  ISIC_1262887        65.0  male     posterior torso                    6.98   \n",
       "3  ISIC_9555133        55.0  male     lower extremity                    9.50   \n",
       "4  ISIC_2851454        60.0  male     posterior torso                    3.97   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('../data/processed/train-metadata.csv')\n",
    "validation = pd.read_csv('../data/processed/validation-metadata.csv')\n",
    "test = pd.read_csv('../data/processed/test-metadata.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c141641-dc3c-4574-bdc6-38ea2396f0fa",
   "metadata": {},
   "source": [
    "## Handle data imbalance in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62780b0f-c7bb-48cc-944b-1fd774c51374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    99.902045\n",
       "1     0.097955\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution After Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    67.105263\n",
       "1    32.894737\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'train' is your DataFrame with the target column 'target'\n",
    "\n",
    "try:\n",
    "    # Print class distribution before sampling\n",
    "    print(\"Class Distribution Before Sampling (%):\")\n",
    "    display(train.target.value_counts(normalize=True) * 100)\n",
    "\n",
    "    # Check if the 'target' column exists in the DataFrame\n",
    "    if 'target' not in train.columns:\n",
    "        raise KeyError(\"The 'target' column is not found in the DataFrame.\")\n",
    "\n",
    "    # Sampling process\n",
    "    try:\n",
    "        # Sample the majority class (0) with a fraction of 0.01\n",
    "        majority_df = train.query(\"target == 0\").sample(frac=0.01, random_state=42)  # Fixed random seed for reproducibility\n",
    "        \n",
    "        # Sample the minority class (1) with a factor of 5.0, allowing replacement\n",
    "        minority_df = train.query(\"target == 1\").sample(frac=5.0, replace=True, random_state=42)\n",
    "        \n",
    "        # Combine the sampled data into a new balanced DataFrame\n",
    "        train_balanced = pd.concat([majority_df, minority_df], axis=0).sample(frac=1.0, random_state=42)  # Shuffle the combined DataFrame\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Error during sampling: {e}\")\n",
    "\n",
    "    # Print class distribution after sampling\n",
    "    print(\"\\nClass Distribution After Sampling (%):\")\n",
    "    display(train_balanced.target.value_counts(normalize=True) * 100)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620e987d-89a7-4e80-8946-12838cb839cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([5.0049e-01, 5.1044e+02])\n"
     ]
    }
   ],
   "source": [
    "# Set class weight for training purpose \n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train['target']), y=train['target'])\n",
    "class_weights = torch.tensor(class_weights,dtype=torch.float)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85baed8-aa60-481f-b258-9bd9debf5543",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a653f05e-6bb7-4491-986a-8b010190b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for handling missing values\n",
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    # Fit method, not modifying any parameters, just returning self\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    # Transform method to handle missing values\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "\n",
    "        # Identify numerical columns\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        # Identify categorical columns\n",
    "        cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        # Create imputer for numerical data using median\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        # Apply imputer to numerical columns\n",
    "        X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "\n",
    "        # Create imputer for categorical data using the most frequent value\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        # Apply imputer to categorical columns\n",
    "        X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "        return X  # Return the transformed DataFrame\n",
    "\n",
    "\n",
    "# Custom transformer for one-hot encoding\n",
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the OneHotEncoder with specified parameters\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "    # Fit method to learn the categories for encoding\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "        # Fit the encoder to categorical columns\n",
    "        self.encoder.fit(X.select_dtypes(include=['object', 'category']))\n",
    "        return self\n",
    "\n",
    "    # Transform method to apply one-hot encoding\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "\n",
    "        # Transform categorical columns to one-hot encoding\n",
    "        encoded_cols = self.encoder.transform(X.select_dtypes(include=['object', 'category']))\n",
    "        # Get the new column names after encoding\n",
    "        new_columns = self.encoder.get_feature_names_out(X.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "        # Create a DataFrame for the encoded columns\n",
    "        encode_df = pd.DataFrame(encoded_cols, columns=new_columns, index=X.index)\n",
    "        # Concatenate the original DataFrame (excluding categorical columns) with the encoded DataFrame\n",
    "        return pd.concat([X.select_dtypes(exclude=['object', 'category']), encode_df], axis=1)\n",
    "\n",
    "# Custom transformer for scaling numerical features\n",
    "class NumericalScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the StandardScaler for scaling numerical features\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    # Fit method to learn the scaling parameters\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "        # Identify numerical columns\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        # Fit the scaler to the numerical columns\n",
    "        self.scaler.fit(X[num_cols])\n",
    "        return self\n",
    "\n",
    "    # Transform method to apply scaling\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "\n",
    "        # Identify numerical columns\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        # Apply scaling to the numerical columns\n",
    "        X[num_cols] = self.scaler.transform(X[num_cols])\n",
    "        return X  # Return the scaled DataFrame\n",
    "\n",
    "# Custom transformer for handling age approximation\n",
    "class AgeApproxTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting required for this transformer\n",
    "\n",
    "    # Transform method to round age approximations\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "        # Check if 'age_approx' is in the DataFrame\n",
    "        if 'age_approx' in X.columns:\n",
    "            # Round the age and convert to integer type\n",
    "            X['age_approx'] = X['age_approx'].round().astype('Int64')\n",
    "        return X  # Return the transformed DataFrame\n",
    "\n",
    "# Create the complete pipeline for preprocessing\n",
    "def process_pipeline() -> Pipeline:\n",
    "    # Define a pipeline with the specified transformers\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('age_transformer', AgeApproxTransformer()),  # Age approximation\n",
    "        ('missing_value_handler', MissingValueHandler()),  # Handling missing values\n",
    "        ('num_scaler', NumericalScaler()),  # Scaling numerical features\n",
    "        ('cat_encoder', OneHotEncoderTransformer())  # One-hot encoding categorical features\n",
    "    ])\n",
    "    return pipeline  # Return the constructed pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b55f33-fb6c-4474-b754-11eaf3e296c1",
   "metadata": {},
   "source": [
    "## Transform train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4734f4a5-c489-47cf-a8c6-435c5c3111a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate case id and target variable from dependable variables\n",
    "X_train = train_balanced.drop(columns=['isic_id','target'])\n",
    "temp_train = train_balanced[['target','isic_id']]\n",
    "pipeline = process_pipeline()\n",
    "train_processed_df = pd.concat([pipeline.fit_transform(X_train),temp_train],axis=1)\n",
    "\n",
    "# Process validation data\n",
    "X_validation = validation.drop(columns=['isic_id', 'target'])\n",
    "temp_validation = validation[['target', 'isic_id']]\n",
    "validation_processed_df = pd.concat([pipeline.transform(X_validation), temp_validation], axis=1)\n",
    "\n",
    "# Process test data\n",
    "X_test = test.drop(columns=['isic_id', 'target'])\n",
    "temp_test = test[['target', 'isic_id']]\n",
    "test_processed_df = pd.concat([pipeline.transform(X_test), temp_test], axis=1)\n",
    "\n",
    "# Save the processed dataframes\n",
    "train_processed_df.to_csv('../data/processed/processed-train-metadata.csv', index=False)\n",
    "validation_processed_df.to_csv('../data/processed/processed-validation-metadata.csv', index=False)\n",
    "test_processed_df.to_csv('../data/processed/processed-test-metadata.csv', index=False)\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f270ea2-c62a-4070-985e-ed60ea36cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading images from HDF5 files\n",
    "class HDF5ImageDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, csv_file, transform=None):\n",
    "        # Open the HDF5 file with error handling\n",
    "        try:\n",
    "            self.hdf5_file = h5py.File(hdf5_file, 'r')  # Read-only mode\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Could not open HDF5 file: {hdf5_file}. Error: {e}\")\n",
    "\n",
    "        # Read the CSV file containing image labels and IDs\n",
    "        try:\n",
    "            self.labels_df = pd.read_csv(csv_file)\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Could not read CSV file: {csv_file}. Error: {e}\")\n",
    "\n",
    "        # Ensure that all image IDs from the CSV are present in the HDF5 file\n",
    "        self.image_ids = self.labels_df['isic_id'].values\n",
    "        for image_id in self.image_ids:\n",
    "            if str(image_id) not in self.hdf5_file.keys():\n",
    "                raise ValueError(f\"Image id {image_id} not found in HDF5 file.\")\n",
    "\n",
    "        # Store any transformations to be applied to the images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image ID from the CSV file based on index\n",
    "        image_id = str(self.labels_df.iloc[idx]['isic_id'])\n",
    "\n",
    "        # Load the image data from the HDF5 file\n",
    "        try:\n",
    "            image_bytes = self.hdf5_file[image_id][()]  # Read image bytes\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Image id {image_id} not found in HDF5 file during __getitem__.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image id {image_id} from HDF5 file. Error: {e}\")\n",
    "\n",
    "        # Convert the image bytes to a PIL Image\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(image_bytes))  # Use io.BytesIO to handle bytes\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Could not convert image bytes to PIL Image. Error: {e}\")\n",
    "\n",
    "        # Apply any specified transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Retrieve the corresponding label for the image\n",
    "        label = self.labels_df.iloc[idx]['target']\n",
    "\n",
    "        return image, label  # Return the image and its label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee53d61c-0744-4422-9cc8-1bd842f312ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths to the HDF5 and CSV files\n",
    "    hdf5_file = '../data/raw/train-image.hdf5'\n",
    "    csv_file = '../data/processed/processed-train-metadata.csv'\n",
    "\n",
    "    # Optionally define any transformations (e.g., resize, normalize, etc.)\n",
    "    from torchvision import transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Example resize\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x / 255.0),\n",
    "    ])\n",
    "\n",
    "    # Create Dataset instance\n",
    "    dataset = HDF5ImageDataset(hdf5_file=hdf5_file, csv_file=csv_file, transform=transform)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8205e8b9-9ee1-47c2-8f15-013747d01864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n",
      "Image batch shape: torch.Size([32, 3, 128, 128]), Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#check if the class works\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Labels batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print shapes\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mHDF5ImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Load the image data from the HDF5 file\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     image_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdf5_file\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Read image bytes\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in HDF5 file during __getitem__.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/h5py/_hl/dataset.py:823\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    821\u001b[0m     arr \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mzeros(selection\u001b[38;5;241m.\u001b[39mmshape, dtype\u001b[38;5;241m=\u001b[39mnew_dtype)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mspace, fspace \u001b[38;5;129;01min\u001b[39;00m selection:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selection\u001b[38;5;241m.\u001b[39mmshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr[()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#check if the class works\n",
    "for images, labels in dataloader:\n",
    "    print(f\"Image batch shape: {images.shape}, Labels batch shape: {labels.shape}\")  # Print shapes\n",
    "    break  # Remove this line to iterate through the entire DataLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
