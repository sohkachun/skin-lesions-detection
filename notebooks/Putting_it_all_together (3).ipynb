{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc82ad-b4a8-44c1-a9aa-3128957931b1",
   "metadata": {
    "id": "f7fc82ad-b4a8-44c1-a9aa-3128957931b1"
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning and PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "\n",
    "# File Handling\n",
    "import h5py\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Progress Visualization\n",
    "from tqdm import tqdm\n",
    "\n",
    "#sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc067a4c-e570-424c-b7b7-90f6f2778eef",
   "metadata": {},
   "source": [
    "# 1) Problem Statement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6439b4-4238-4203-ae8c-b0f5ee9dca75",
   "metadata": {},
   "source": [
    "#### Skin cancer is the most common form of cancer in the United States and ranks 17th globally (WCRF).There are three major types of skin cancerâ€”Basal Cell Carcinoma (BCC), Squamous Cell Carcinoma (SCC), and Melanoma. While BCC and SCC are considered less lethal, melanoma is the deadliest form ofskin cancer. It is expected to be diagnosed over 200,000 times in the US in 2024, with nearly 9,000 deathsprojected. Automated image analysis tools play a significant role in expediting clinical presentation anddiagnosis, positively impacting hundreds of thousands of people each year.For a telehealth app company, addressing the challenge of skin cancer detection in underserved populations or non-clinical settings is particularly significant. Current diagnostic methods rely on high-quality dermatoscope images, which are typically captured in dermatology clinics. These images reveal morphological features not visible to the naked eye. To provide this early detection service on our platform, we need to develop an algorithm capable of accurately classifying lower-quality malignant skin lesions from benign ones. Additionally, this algorithm should assist in diagnosing users based on their type of lesions and personal information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80700ce7-ab80-45b9-a855-416b9bb9e223",
   "metadata": {},
   "source": [
    "# 2) Data Ingestion\n",
    "#### From the Data Ingestion to Data Preprocessing stage, I utilized the original dataset prior to resampling, which is too large to upload to GitHub. As a result, you may encounter an error stating \"No such file exists.\" To address this limitation, I discussed the issue of data size constraints with the professor. Subsequently, after resampling the dataset, I proceeded with data preprocessing and used the resampled data for the subsequent stages, including Feature Engineering and Model Development. This approach allowed me to handle the imbalanced dataset effectively while aligning with the constraints of data storage and accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958359d-b308-483c-ae64-b44f16e1344d",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40834f8-a3cd-487d-8db9-214e11b4132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/raw/train-metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71a8dd-5bc6-4b4c-88cf-dba7978944b8",
   "metadata": {},
   "source": [
    "# 3) Explolatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f559baee-b0c7-4245-898a-80b64e840658",
   "metadata": {},
   "source": [
    "## Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c23272-8e37-47b0-96e6-7c9a8790469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(df: pd.DataFrame, include_all: bool = False):\n",
    "    \"\"\"\n",
    "    Print statistics and null value counts for a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        include_all (bool): If True, include all columns in the descriptive statistics; otherwise, include only numeric columns.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"The DataFrame is empty.\")\n",
    "        return\n",
    "\n",
    "    # Print descriptive statistics\n",
    "    print(\"Descriptive Statistics:\")\n",
    "    if include_all:\n",
    "        print(df.describe(include='all'))\n",
    "    else:\n",
    "        print(df.describe(include=[np.number]))\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for clarity\n",
    "\n",
    "    # Print the number of null values per column\n",
    "    print(\"Null Value Counts:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for clarity\n",
    "\n",
    "    # Additional information: Percentage of null values per column\n",
    "    print(\"Percentage of Null Values:\")\n",
    "    print(df.isnull().mean() * 100)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for clarity\n",
    "\n",
    "    # Number of rows and columns\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2033735-e924-473c-a418-c1c8a6b00df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cc1d1-7da4-4d35-93b6-831522407c8d",
   "metadata": {},
   "source": [
    "#### In the application I am developing, users will input an image and provide personal information. To ensure transparency and a user-centric design, only metadata accessible to users will be used as predictors in the model. Consequently, I have selected the following metadata fields for inclusion: \"age_approx\", \"sex\", \"anatom_site_general\", and \"clin_size_long_diam_mm\". These fields are both relevant to the prediction task and available to users.\n",
    "\n",
    "#### From the data analysis, \"age_approx\", \"sex\", and \"anatom_site_general\" were identified as having missing values. However, the percentage of missing data for these fields is manageable, allowing for imputation strategies such as using the median for numerical fields like \"age_approx\" and the mode for categorical fields like \"sex\" and \"anatom_site_general.\" This ensures the completeness and reliability of the metadata while maintaining the model's predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967106e-6708-4726-b94c-653d0928982c",
   "metadata": {},
   "source": [
    "## Visualize Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6acae-fa94-4419-bd54-aa06f631ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Distribution\n",
    "\n",
    "# Count the occurrences of each target value and sort by index\n",
    "target_counts = data['target'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the total number of samples in the training DataFrame\n",
    "total = len(data)\n",
    "\n",
    "# Create a list of percentages for each target class, formatted as a string\n",
    "percentage = [f'{count/total:0.3%}' for count in target_counts]\n",
    "\n",
    "# Create a bar plot to visualize the distribution of the target variable\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=target_counts.index,  # X-axis represents the unique target classes\n",
    "        y=target_counts.values,  # Y-axis represents the counts of each class\n",
    "        text=percentage,  # Display percentages on top of the bars\n",
    "        textposition='auto'  # Automatically position text on bars\n",
    "    )\n",
    "])\n",
    "\n",
    "# Update layout of the plot with titles and formatting\n",
    "fig.update_layout(\n",
    "    title='Distribution of Target Variable',  # Main title of the plot\n",
    "    xaxis_title='Lesion Class',  # Title for the X-axis\n",
    "    yaxis_title='Count',  # Title for the Y-axis\n",
    "    template='plotly_white',  # Use a white background for the plot\n",
    "    height=600, width=1200  # Set the dimensions of the plot\n",
    ")\n",
    "\n",
    "# Set the y-axis to a logarithmic scale to better visualize class distributions\n",
    "fig.update_layout(yaxis=dict(type='log'))\n",
    "\n",
    "# Add an annotation to show the total number of samples in the dataset\n",
    "fig.add_annotation(\n",
    "    text=f\"<b>TOTAL SAMPLES:  {total:,}</b>\",  # Format total count with commas\n",
    "    xref=\"paper\", yref=\"paper\",  # Reference the entire paper for positioning\n",
    "    x=0.98, y=1.05,  # Position the annotation near the top-right corner\n",
    "    showarrow=False,  # Do not show an arrow pointing to the text\n",
    "    font=dict(size=12)  # Set the font size for the annotation\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8b853-a7a8-425f-9239-28fdc168e77b",
   "metadata": {},
   "source": [
    "#### From the target distribution graph, it is evident that the dataset is highly imbalanced. Class 1, representing malignant cases, constitutes only 0.098% of the total data, while Class 0, representing benign cases, accounts for 99.902%. This extreme imbalance poses challenges for the model, as it may struggle to adequately learn patterns for the minority class, potentially leading to biased predictions heavily favoring the majority class. Addressing this imbalance is crucial to ensure the model's effectiveness and fairness, particularly for detecting malignant cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eadd4a-5668-40ab-bd1e-bf6e08d27262",
   "metadata": {},
   "source": [
    "## Visualize categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78d26f-67d3-4844-83b4-7311846d282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_categorical_feature_distribution(\n",
    "    df: pd.DataFrame,\n",
    "    feature_col: str,\n",
    "    target_col: str = 'target',\n",
    "    target_as_str: bool = True,\n",
    "    log_y: bool = False,\n",
    "    template_theme: str = \"plotly_white\",\n",
    "    group_by_target: bool = True,\n",
    "    stack_bar: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the distribution of a categorical feature, optionally grouped by a target variable.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        feature_col (str): The name of the categorical feature column to plot.\n",
    "        target_col (str, optional): The name of the target column. Defaults to 'target'.\n",
    "        target_as_str (bool, optional): Whether to treat target variable as strings. Defaults to True.\n",
    "        log_y (bool, optional): Whether to use a logarithmic scale for the Y-axis. Defaults to False.\n",
    "        template_theme (str, optional): Plotly template theme to use. Defaults to 'plotly_white'.\n",
    "        group_by_target (bool, optional): Whether to group bars by target variable. Defaults to True.\n",
    "        stack_bar (bool, optional): Whether to stack bars instead of grouping. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None; displays the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the DataFrame and sort it based on feature and target columns\n",
    "    _df = df.copy().sort_values(by=[feature_col, target_col]).reset_index(drop=True)\n",
    "\n",
    "    # Check if we need to group the bars by the target variable\n",
    "    if group_by_target:\n",
    "        # Create a histogram plot grouped by the target variable\n",
    "        fig = px.histogram(\n",
    "            _df, x=feature_col, color=target_col,\n",
    "            log_y=log_y, height=500, width=1200, template=template_theme,\n",
    "            title=f'Distribution of {feature_col.upper()} By TARGET',\n",
    "            barmode='group' if not stack_bar else 'stack'  # Choose between grouped or stacked bars\n",
    "        )\n",
    "    else:\n",
    "        # Create a histogram plot without grouping by the target variable\n",
    "        fig = px.histogram(\n",
    "            _df, x=feature_col, color=feature_col, \n",
    "            log_y=log_y, height=500, width=1200, template=template_theme,\n",
    "            title=f'<b>DISTRIBUTION OF {feature_col.replace(\"_\", \" \").upper()}',\n",
    "        )  \n",
    "\n",
    "    # Update the layout of the plot with titles and gaps\n",
    "    fig.update_layout(\n",
    "        bargap=0.1,  # Set the gap between bars\n",
    "        xaxis_title=f\"{feature_col.title()}\",  # Format the X-axis title\n",
    "        yaxis_title=\"Count\",  # Title for the Y-axis\n",
    "        showlegend=group_by_target  # Show legend only when grouped by target\n",
    "    )\n",
    "\n",
    "    # Apply log scale to the Y-axis if requested\n",
    "    if log_y:\n",
    "        fig.update_layout(yaxis_type=\"log\")\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c57412-d12b-4b28-8175-4736974c9a38",
   "metadata": {},
   "source": [
    "### Age_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f5ec1-4b93-4ddb-a345-2e4532a0f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_feature_distribution(data, \"age_approx\", group_by_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27771a2-e502-4f30-99ff-3ffa8ca425f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_feature_distribution(data, \"age_approx\", group_by_target=True, stack_bar=False, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7aee7-efd2-49b3-8938-fc022f3e4e67",
   "metadata": {},
   "source": [
    "### Anatom_Site_General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2a243-dcd7-439e-9e91-0f48ee1cbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_feature_distribution(data, \"anatom_site_general\", group_by_target=True, stack_bar=False, log_y = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce8892-8810-4b46-bd5a-a0f8637bbe85",
   "metadata": {},
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836643e-fe09-4594-a24c-529316534c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_feature_distribution(data, \"sex\", group_by_target=True, stack_bar=False, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb939b-775a-41ce-b866-2cacdc367781",
   "metadata": {},
   "source": [
    "#### From these graphs, I found out that age groups under 40 and females, in particular, are underrepresented for malignant cases. This could lead to lower recall for these subgroups, as the model may not learn enough from the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050640e-b718-419d-b69b-f3fd7ce890aa",
   "metadata": {},
   "source": [
    "## Visualize continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923665b-b5e0-427a-b464-81b714322247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_continuous_feature_distribution(\n",
    "    df: pd.DataFrame, \n",
    "    feature_col: str,\n",
    "    plot_style: str = \"histogram\",\n",
    "    feature_readable_name: str | None = None,\n",
    "    target_col: str = \"target\",\n",
    "    log_y: bool = False, \n",
    "    template_theme: str = \"plotly_white\",\n",
    "    group_by_target: bool = True,\n",
    "    n_bins: int = 50\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the distribution of a continuous feature in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the feature and target columns.\n",
    "        feature_col (str): The name of the feature column to plot.\n",
    "        plot_style (str, optional): The style of the plot ('histogram' or 'box'). Defaults to 'histogram'.\n",
    "        feature_readable_name (str | None, optional): A readable name for the feature to use in the title. Defaults to None.\n",
    "        target_col (str, optional): The name of the target column. Defaults to 'target'.\n",
    "        log_y (bool, optional): Whether to apply a logarithmic scale to the y-axis. Defaults to False.\n",
    "        template_theme (str, optional): The Plotly template theme to use for the plot. Defaults to 'plotly_white'.\n",
    "        group_by_target (bool, optional): Whether to group the plot by the target variable. Defaults to True.\n",
    "        n_bins (int, optional): The number of bins to use for the histogram. Defaults to 50.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If df is not a pandas DataFrame.\n",
    "        ValueError: If feature_col or target_col are not found in the DataFrame or if plot_style is invalid.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
    "    if feature_col not in df.columns:\n",
    "        raise ValueError(f\"Feature column '{feature_col}' not found in DataFrame.\")\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in DataFrame.\")\n",
    "    if plot_style not in [\"histogram\", \"box\"]:\n",
    "        raise ValueError(\"Invalid plot_style. Choose either 'histogram' or 'box'.\")\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid modifying the original data\n",
    "    _df = df.copy().sort_values(by=[feature_col, target_col]).reset_index(drop=True)\n",
    "\n",
    "    # Plotting logic based on the chosen plot style\n",
    "    if plot_style == \"histogram\":\n",
    "        if group_by_target:\n",
    "            # Create a histogram for each target value\n",
    "            fig = go.Figure()\n",
    "            for target_value in _df[target_col].unique():\n",
    "                subset = _df[_df[target_col] == target_value]\n",
    "                fig.add_trace(go.Histogram(\n",
    "                    x=subset[feature_col],\n",
    "                    name=str(target_value),\n",
    "                    opacity=0.7,\n",
    "                    nbinsx=n_bins\n",
    "                ))\n",
    "\n",
    "            # Update layout for overlay histogram\n",
    "            fig.update_layout(\n",
    "                barmode='overlay',\n",
    "                title=f\"Distribution of {feature_readable_name or feature_col.upper()} by Target\",\n",
    "                height=500, width=1200, template=template_theme,\n",
    "                xaxis_title=feature_readable_name or feature_col,\n",
    "                yaxis_title=\"Count\",\n",
    "                showlegend=True\n",
    "            )\n",
    "        else:\n",
    "            # Create a single histogram without grouping\n",
    "            fig = px.histogram(\n",
    "                _df, x=feature_col, log_y=log_y, height=500, width=1200, template=template_theme,\n",
    "                title=f\"Distribution of {feature_readable_name or feature_col.upper()}\",\n",
    "                nbins=n_bins\n",
    "            )\n",
    "\n",
    "            # Update layout for single histogram\n",
    "            fig.update_layout(\n",
    "                xaxis_title=feature_readable_name or feature_col,\n",
    "                yaxis_title=\"Count\",\n",
    "                showlegend=False\n",
    "            )\n",
    "\n",
    "    elif plot_style == \"box\":\n",
    "        if group_by_target:\n",
    "            # Create a box plot for each target value\n",
    "            fig = go.Figure()\n",
    "            for target_value in _df[target_col].unique():\n",
    "                subset = _df[_df[target_col] == target_value]\n",
    "                fig.add_trace(go.Box(\n",
    "                    y=subset[feature_col],\n",
    "                    name=str(target_value),\n",
    "                    boxpoints='outliers',  # Show outliers\n",
    "                    boxmean=True  # Show mean in the box plot\n",
    "                ))\n",
    "\n",
    "            # Update layout for box plot grouped by target\n",
    "            fig.update_layout(\n",
    "                title=f'Distribution of {feature_readable_name or feature_col.upper()} by Target (includes likely outliers)', \n",
    "                height=500, width=1200, template=template_theme,\n",
    "                xaxis_title='Target',\n",
    "                yaxis_title=f'{feature_readable_name or feature_col}',\n",
    "                showlegend=True\n",
    "            )\n",
    "        else:\n",
    "            # Create a single box plot without grouping\n",
    "            fig = px.box(\n",
    "                _df, y=feature_col,\n",
    "                height=500,\n",
    "                width=1200,\n",
    "                template=template_theme,\n",
    "                title=f\"Distribution of {feature_readable_name or feature_col.upper()}\",\n",
    "                points=\"outliers\",  # Show outliers\n",
    "            )\n",
    "                             \n",
    "            # Update layout for single box plot\n",
    "            fig.update_layout(\n",
    "                yaxis_title=f'{feature_readable_name or feature_col}',\n",
    "                showlegend=False\n",
    "            )   \n",
    "\n",
    "    # Apply log scale to y-axis if requested (only for histogram)\n",
    "    if log_y and plot_style == \"histogram\":\n",
    "        fig.update_layout(yaxis_type='log')\n",
    "    \n",
    "    # Display the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec86877-b049-4c3e-b22e-c02c26fc5403",
   "metadata": {},
   "source": [
    "### clin_size_long_diam_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1809b12-b971-447e-aae7-22c307f58555",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous_feature_distribution(data, 'clin_size_long_diam_mm', plot_style=\"box\", log_y=True, group_by_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e58eed-c034-4fc2-a2a4-837fa919a81b",
   "metadata": {},
   "source": [
    "#### The boxplot above shows significant outliers in the \"clin_size_long_diam_mm\" feature for both classes, especially Class 0. These outliers can negatively impact the training of a neural network by skewing the weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd9afb-b13e-4353-88c8-3bce67752a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous_feature_distribution(data, 'clin_size_long_diam_mm', plot_style=\"histogram\", log_y=True, group_by_target=True, n_bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718129c-8a17-45d5-b309-7a38b8f8bb2a",
   "metadata": {},
   "source": [
    "## Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db9fe8-19f7-494c-83f5-263110c01a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load image from hdf5 file\n",
    "def load_image_from_hdf5(isic_id: str,\n",
    "                         file_path: str = \"../data/raw/train-image.hdf5\",\n",
    "                         n_channels: int = 3):\n",
    "    # Handle the case where the isic_id is passed incorrectly\n",
    "    if not isic_id.lower().startswith(\"isic\"):\n",
    "        isic_id = f\"ISIC_{int(str(isic_id).split('_', 1)[-1]):>07}\"\n",
    "        \n",
    "    # Open the HDF5 file in read mode\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        \n",
    "        # Retrieve the image data from the HDF5 dataset using the provided ISIC ID\n",
    "        try:\n",
    "            image_data = hf[isic_id][()]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"ISIC ID {isic_id} not found in HDF5 file.\")\n",
    "\n",
    "        # Convert the binary data to a numpy array\n",
    "        image_array = np.frombuffer(image_data, np.uint8)\n",
    "\n",
    "        # Decode the image from the numpy array\n",
    "        if n_channels == 3:\n",
    "            # Load the image as a color image (BGR) and convert to RGB\n",
    "            image = cv2.cvtColor(cv2.imdecode(image_array, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            # Load the image as a grayscale image\n",
    "            image = cv2.imdecode(image_array, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # If the image failed to load for some reason (problems decoding) ...\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not decode image for ISIC ID: {isic_id}\")\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b17ca-5c0c-4aac-817d-7bfdef753256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_by_target(df: pd.DataFrame, target_value: int, max_images: int = 10) -> None:\n",
    "    \"\"\"Load and plot images based on the target value.\n",
    "\n",
    "    Args:\n",
    "        processed_df (pd.DataFrame): The DataFrame containing image metadata.\n",
    "        target_value (int): The target value to filter images.\n",
    "        max_images (int, optional): Maximum number of images to display. Defaults to 10.\n",
    "    \n",
    "    Returns:\n",
    "        None; displays a plot of the images.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not isinstance(target_value, int):\n",
    "        raise ValueError(\"target_value must be an integer.\")\n",
    "    if not isinstance(max_images, int) or max_images <= 0:\n",
    "        raise ValueError(\"max_images must be a positive integer.\")\n",
    "\n",
    "    # Filter the DataFrame for the specified target value and limit the number of images\n",
    "    filtered_df = df[df['target'] == target_value].head(max_images)\n",
    "\n",
    "    images = []  # Initialize a list to hold the loaded images\n",
    "    for isic_id in filtered_df['isic_id']:\n",
    "        try:\n",
    "            # Load the image using the provided ISIC ID from the HDF5 file\n",
    "            image = load_image_from_hdf5(isic_id)\n",
    "            images.append(image)  # Append the loaded image to the list\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image for ISIC ID {isic_id}: {e}\")\n",
    "\n",
    "    # Create a DataFrame to store the loaded images along with their metadata\n",
    "    image_df = pd.DataFrame({\n",
    "        'isic_id': filtered_df['isic_id'],\n",
    "        'target': filtered_df['target'],\n",
    "        'image': images\n",
    "    })\n",
    "\n",
    "    n_images = len(image_df)  # Get the number of images to display\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(15, 5))  # Create a subplot for each image\n",
    "    fig.suptitle(f'Images of Lesions with Target Value {target_value}', fontsize=14)  # Main title\n",
    "\n",
    "    # Iterate over the axes, ISIC IDs, and images to display each image\n",
    "    for ax, isic_id, img in zip(axes, image_df['isic_id'], image_df['image']):\n",
    "        ax.imshow(img)  # Display the image\n",
    "        ax.set_title(f'ISIC ID: {isic_id}', fontsize=5)  # Set the title for each image\n",
    "        ax.axis('off')  # Hide the axis\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to make room for the main title\n",
    "    plt.show()  # Display the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd440c94-05f3-4ea1-a549-139bf1a7c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_by_target(data, target_value=1, max_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51916ea6-3307-41bd-ba89-af647c62ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_by_target(data, target_value=0, max_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfc9f7-6495-456e-b0d0-119213db484c",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "#### To perform correlation analysis, I must first split the dataset into training, validation, and test sets. The training data is then used for the correlation analysis due to computational constraints that prevent me from using the entire dataset. By focusing on the training data, I ensure that the subset adequately represents the overall dataset while remaining manageable for analysis. This allows for meaningful computation of correlation coefficients and effective visualization of feature relationships using a heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb4f30-fa6e-4fe7-87bf-bff229b25479",
   "metadata": {},
   "source": [
    "### Split Data inot Train, Validation and Test\n",
    "#### I split the data into 70% train, 15% validation and 15% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c7709-a1ed-49b7-997f-ca3c23eaa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from the \"train-metadata.csv file\"  and split into train, val, test\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv('../data/raw/train-metadata.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The specified CSV file was not found.\")\n",
    "    raise  # Re-raise the error after logging\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The CSV file is empty.\")\n",
    "    raise\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The CSV file could not be parsed.\")\n",
    "    raise\n",
    "\n",
    "# Select features (X) and the target variable (y)\n",
    "try:\n",
    "    X = data[['isic_id', 'age_approx', 'sex', 'anatom_site_general', 'clin_size_long_diam_mm']]\n",
    "    y = data['target']\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing expected column in the dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Split the data into training and temporary sets (70% train, 30% temp)\n",
    "try:\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        random_state=88, \n",
    "        stratify=y  # Ensures the target variable distribution is preserved\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Error during train-test split: {e}\")\n",
    "    raise\n",
    "\n",
    "# Further split the temporary set into validation and test sets (15% val, 15% test)\n",
    "try:\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size=0.5,  # This effectively splits the 30% temp into two equal parts\n",
    "        random_state=88, \n",
    "        stratify=y_temp  # Again preserves the target variable distribution\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Error during validation-test split: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create DataFrames for the training, validation, and test sets\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "validation_df = pd.concat([X_val, y_val], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save the processed DataFrames to CSV files\n",
    "try:\n",
    "    train_df.to_csv('../data/processed/train-metadata.csv', index=False)\n",
    "    validation_df.to_csv('../data/processed/validation-metadata.csv', index=False)\n",
    "    test_df.to_csv('../data/processed/test-metadata.csv', index=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error while saving CSV files: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b829f-a980-4637-8033-620e55ce1307",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20cefc-eb3c-49fb-9a12-a5eed952ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for handling missing values\n",
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    # Fit method, not modifying any parameters, just returning self\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    # Transform method to handle missing values\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "\n",
    "        # Identify numerical columns\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        # Identify categorical columns\n",
    "        cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        # Create imputer for numerical data using median\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        # Apply imputer to numerical columns\n",
    "        X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "\n",
    "        # Create imputer for categorical data using the most frequent value\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        # Apply imputer to categorical columns\n",
    "        X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
    "\n",
    "        return X  # Return the transformed DataFrame\n",
    "\n",
    "# Custom transformer for one-hot encoding\n",
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the OneHotEncoder with specified parameters\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "    # Fit method to learn the categories for encoding\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "        # Fit the encoder to categorical columns\n",
    "        self.encoder.fit(X.select_dtypes(include=['object', 'category']))\n",
    "        return self\n",
    "\n",
    "    # Transform method to apply one-hot encoding\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "\n",
    "        # Transform categorical columns to one-hot encoding\n",
    "        encoded_cols = self.encoder.transform(X.select_dtypes(include=['object', 'category']))\n",
    "        # Get the new column names after encoding\n",
    "        new_columns = self.encoder.get_feature_names_out(X.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "        # Create a DataFrame for the encoded columns\n",
    "        encode_df = pd.DataFrame(encoded_cols, columns=new_columns, index=X.index)\n",
    "        # Concatenate the original DataFrame (excluding categorical columns) with the encoded DataFrame\n",
    "        return pd.concat([X.select_dtypes(exclude=['object', 'category']), encode_df], axis=1)\n",
    "\n",
    "# Custom transformer for scaling numerical features\n",
    "class NumericalScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the StandardScaler for scaling numerical features\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    # Fit method to learn the scaling parameters\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "        # Identify numerical columns\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        # Fit the scaler to the numerical columns\n",
    "        self.scaler.fit(X[num_cols])\n",
    "        return self\n",
    "\n",
    "    # Transform method to apply scaling\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "\n",
    "        # Identify numerical columns\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        # Apply scaling to the numerical columns\n",
    "        X[num_cols] = self.scaler.transform(X[num_cols])\n",
    "        return X  # Return the scaled DataFrame\n",
    "\n",
    "# Custom transformer for handling age approximation\n",
    "class AgeApproxTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting required for this transformer\n",
    "\n",
    "    # Transform method to round age approximations\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a pandas DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "        # Check if 'age_approx' is in the DataFrame\n",
    "        if 'age_approx' in X.columns:\n",
    "            # Round the age and convert to integer type\n",
    "            X['age_approx'] = X['age_approx'].round().astype('Int64')\n",
    "        return X  # Return the transformed DataFrame\n",
    "\n",
    "# Create the complete pipeline for preprocessing\n",
    "def create_pipeline() -> Pipeline:\n",
    "    # Define a pipeline with the specified transformers\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('age_transformer', AgeApproxTransformer()),  # Age approximation\n",
    "        ('missing_value_handler', MissingValueHandler()),  # Handling missing values\n",
    "        ('cat_encoder', OneHotEncoderTransformer()),  # One-hot encoding categorical features\n",
    "        ('num_scaler', NumericalScaler())  # Scaling all numerical features (including encoded features)\n",
    "    ])\n",
    "    return pipeline  # Return the constructed pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959531b-3c20-4d40-915a-d078270ed75c",
   "metadata": {},
   "source": [
    "#### After week 4, I realized that applying StandardScaler to my dataset may not be the optimal choice for a neural network model. Instead, using MinMaxScaler is more appropriate, as it scales the data to a range of 0 to 1, which aligns better with the activation functions commonly used in neural networks. This adjustment ensures that the input features are normalized in a way that enhances the model's learning efficiency and stability. Moving forward, this is one of the changes I will implement to improve the overall performance of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c613b3-50f1-4add-a7ea-f40db59d714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training metadata from a CSV file\n",
    "\n",
    "\n",
    "# Drop the 'target' and 'isic_id' columns to create the feature set\n",
    "X = train_df.drop(columns=['target', 'isic_id'])\n",
    "\n",
    "# Keep the 'target' and 'isic_id' columns in a separate DataFrame for later use\n",
    "temp = train_df[['target', 'isic_id']]\n",
    "\n",
    "# Create the preprocessing pipeline using the previously defined function\n",
    "pipeline = create_pipeline()\n",
    "\n",
    "try:\n",
    "    # Fit the pipeline to the feature set and transform the data\n",
    "    processed_X = pipeline.fit_transform(X)\n",
    "except Exception as e:\n",
    "    # Log any errors that occur during fitting and transformation\n",
    "    print(f\"Error occurred during pipeline processing: {e}\")\n",
    "\n",
    "# Concatenate the processed features with the target and ISIC ID columns\n",
    "processed_df = pd.concat([processed_X, temp], axis=1)\n",
    "\n",
    "# Calculate the correlation matrix, excluding the 'isic_id' column\n",
    "correlation_matrix = processed_df.drop(columns=['isic_id']).corr()\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create the heatmap using seaborn\n",
    "sns.heatmap(\n",
    "    correlation_matrix,            # The correlation matrix to visualize\n",
    "    annot=True,                    # Annotate each cell with the numeric value\n",
    "    fmt=\".2f\",                     # Format the annotation to two decimal places\n",
    "    cmap='coolwarm',               # Color map for the heatmap\n",
    "    square=True                    # Ensure each cell is square-shaped\n",
    ")\n",
    "\n",
    "# Set the title for the plot\n",
    "plt.title('Correlation Matrix Heatmap (Including Target Variable)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c98b4-f505-45e0-9ea9-ccbaf8780d68",
   "metadata": {},
   "source": [
    "#### We can see that sex_female and sex_male are highly correlated, but I do not think it will significantly affect the accuracy of the neural network model because the relationship between these two features is binary and mutually exclusive. In this case, one being 1 automatically implies the other is 0. Neural networks are capable of learning such simple relationships efficiently without causing confusion or overfitting.\n",
    "\n",
    "#### In the future, as a best practice, one of these features could be dropped without any loss of information, as retaining only sex_female (or sex_male) is sufficient to convey the same information. However, for interpretability and domain alignment, keeping both features might be beneficial depending on how the model's results are presented or used. This decision could also depend on how stakeholders prefer to view or analyze the results of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12992a8-2eb4-4185-ab45-06d2101025fd",
   "metadata": {},
   "source": [
    "# 4) Preprocess & Feature Engineer data\n",
    "#### For metadata preprocessing, I will utilize a custom pipeline that includes handling missing values, one-hot encoding categorical variables, and scaling numerical features. This ensures that all metadata inputs are properly formatted for input into the neural network.\n",
    "\n",
    "#### For image feature engineering, I will apply transformations such as resizing, rotation, and random cropping to the images. These transformations help improve model generalization by introducing variability in the training data, thereby reducing the risk of overfitting.\n",
    "\n",
    "#### By combining these preprocessing steps, I aim to ensure that both the metadata and image inputs are in optimal condition for the neural network, leading to better model performance and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96227f2b-6c19-48ec-897e-3d545eb537fb",
   "metadata": {},
   "source": [
    "## Handle data imbalance in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3242b-76cd-43bf-85cb-1bef5207e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train' is your DataFrame with the target column 'target'\n",
    "try:\n",
    "    # Print class distribution before sampling\n",
    "    print(\"Class Distribution Before Sampling (%):\")\n",
    "    display(train_df.target.value_counts(normalize=True) * 100)\n",
    "\n",
    "    # Check if the 'target' column exists in the DataFrame\n",
    "    if 'target' not in train_df.columns:\n",
    "        raise KeyError(\"The 'target' column is not found in the DataFrame.\")\n",
    "\n",
    "    # Sampling process\n",
    "    try:\n",
    "        # Sample the majority class (0) with a fraction of 0.01\n",
    "        majority_df = train_df.query(\"target == 0\").sample(frac=0.01, random_state=42)  # Fixed random seed for reproducibility\n",
    "        \n",
    "        # Sample the minority class (1) with a factor of 5.0, allowing replacement\n",
    "        minority_df = train_df.query(\"target == 1\").sample(frac=5.0, replace=True, random_state=42)\n",
    "        \n",
    "        # Combine the sampled data into a new balanced DataFrame\n",
    "        train_balanced = pd.concat([majority_df, minority_df], axis=0).sample(frac=1.0, random_state=42)  # Shuffle the combined DataFrame\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Error during sampling: {e}\")\n",
    "\n",
    "    # Print class distribution after sampling\n",
    "    print(\"\\nClass Distribution After Sampling (%):\")\n",
    "    display(train_balanced.target.value_counts(normalize=True) * 100)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786e83a-ab9f-456f-9294-6e7934fd1a39",
   "metadata": {},
   "source": [
    "#### As you can see, I have downsized the majority class and upsized the minority class to address the class imbalance in the dataset. This resampling strategy aims to create a more balanced distribution of classes, which can help the model better identify and classify minority class instances.\n",
    "\n",
    "#### One important consideration is that this approach may still affect the model's ability to generalize to new image data and metadata. By artificially altering the class distribution, there is a risk of overfitting to the resampled data, especially if the model becomes too focused on the minority class. To mitigate this, I will employ strategies such as cross-validation, early stopping, and careful selection of evaluation metrics (e.g., AUROC, precision-recall) to ensure the model remains robust on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853b0b7-dbab-446b-81a4-9528cec616f8",
   "metadata": {},
   "source": [
    "## Metadata Preprocessing Pipeline\n",
    "#### The metadata prerpocessing pipeline includes:\n",
    "* #### Impute Missing Value for both categorical and numerical data\n",
    "* #### One-Hot Encode for categorical data\n",
    "* #### Scale numerical variables with StandardScaler\n",
    "* #### Convert age from float to interger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5623f66-15b1-41c4-8feb-624b02b2fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate case id and target variable from dependable variables\n",
    "pipeline = create_pipeline()\n",
    "X_train = train_balanced.drop(columns=['isic_id','target'])\n",
    "temp_train = train_balanced[['target','isic_id']]\n",
    "train_processed_df = pd.concat([pipeline.fit_transform(X_train),temp_train],axis=1)\n",
    "\n",
    "# Process validation data\n",
    "X_validation = validation_df.drop(columns=['isic_id', 'target'])\n",
    "temp_validation = validation_df[['target', 'isic_id']]\n",
    "validation_processed_df = pd.concat([pipeline.transform(X_validation), temp_validation], axis=1)\n",
    "\n",
    "# Process test data\n",
    "X_test = test_df.drop(columns=['isic_id', 'target'])\n",
    "temp_test = test_df[['target', 'isic_id']]\n",
    "test_processed_df = pd.concat([pipeline.transform(X_test), temp_test], axis=1)\n",
    "\n",
    "# Save the processed dataframes\n",
    "train_processed_df.to_csv('../data/processed/processed-train-metadata.csv', index=False)\n",
    "validation_processed_df.to_csv('../data/processed/processed-validation-metadata.csv', index=False)\n",
    "test_processed_df.to_csv('../data/processed/processed-test-metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca12b4-4c47-4f7a-bfd2-90ff351748ed",
   "metadata": {},
   "source": [
    "## Feature Engineer Image Data\n",
    "\n",
    "#### I will create a custom dataset to store and preprocess the data, enabling efficient data loading and feature engineering for later use in the model. This approach ensures that the data is preprocessed consistently and allows for easy access during model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be7e85-2106-4380-b55a-e3ab5e124265",
   "metadata": {
    "id": "84be7e85-2106-4380-b55a-e3ab5e124265"
   },
   "source": [
    "### Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7SW_pt8j6U-j",
   "metadata": {
    "id": "7SW_pt8j6U-j"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiInputDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, csv_file, transform=None):\n",
    "        # Open the HDF5 file with error handling\n",
    "        try:\n",
    "            self.hdf5_file = h5py.File(hdf5_file, 'r')  # Read-only mode\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Could not open HDF5 file: {hdf5_file}. Error: {e}\")\n",
    "\n",
    "        # Read the CSV file containing image labels and additional features\n",
    "        try:\n",
    "            self.labels_df = pd.read_csv(csv_file)\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Could not read CSV file: {csv_file}. Error: {e}\")\n",
    "\n",
    "        # Ensure that all image IDs from the CSV are present in the HDF5 file\n",
    "        self.image_ids = self.labels_df['isic_id'].values\n",
    "        for image_id in self.image_ids:\n",
    "            if str(image_id) not in self.hdf5_file.keys():\n",
    "                raise ValueError(f\"Image id {image_id} not found in HDF5 file.\")\n",
    "\n",
    "        # Store any transformations to be applied to the images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image ID from the CSV file based on index\n",
    "        image_id = str(self.labels_df.iloc[idx]['isic_id'])\n",
    "\n",
    "        # Load the image data from the HDF5 file\n",
    "        image_bytes = self.hdf5_file[image_id][()]\n",
    "\n",
    "        # Convert the image bytes to a PIL Image\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "        # Apply any specified transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Retrieve the label\n",
    "        label = torch.tensor(self.labels_df.iloc[idx]['target'], dtype=torch.long)  # Adjust dtype if needed\n",
    "\n",
    "        # Retrieve other features, excluding 'isic_id' and 'target'\n",
    "        other_variables = self.labels_df.iloc[idx].drop(['isic_id', 'target']).values.astype(float)\n",
    "\n",
    "        # Convert other variables (metadata) to a tensor\n",
    "        metadata_tensor = torch.tensor(other_variables, dtype=torch.float32)\n",
    "\n",
    "        # Return the image, metadata, and label\n",
    "        return image, metadata_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefec3fb-19ff-423c-9658-c02dbc82f656",
   "metadata": {
    "editable": true,
    "id": "fefec3fb-19ff-423c-9658-c02dbc82f656",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engineer for train,validation and test image data\n",
    "\n",
    "def get_train_transform(resize_size=(224, 224), crop_size=128, rotation_degree=10, normalize_means=(0.5, 0.5, 0.5), normalize_stds=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    Returns the transformations for the training dataset, including data augmentation.\n",
    "\n",
    "    Args:\n",
    "        resize_size (tuple): The size to resize the image before cropping.\n",
    "        crop_size (int): The size of the random crop.\n",
    "        rotation_degree (int): Maximum degree for random rotation.\n",
    "        normalize_means (tuple): Means for normalization.\n",
    "        normalize_stds (tuple): Standard deviations for normalization.\n",
    "\n",
    "    Returns:\n",
    "        transforms.Compose: The composed transformations for the training set.\n",
    "    \"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(resize_size),  # Resize to specified size\n",
    "        transforms.RandomResizedCrop(crop_size, scale=(0.8, 1.0)),  # Random crop with scale\n",
    "        transforms.RandomRotation(rotation_degree),  # Randomly rotate images\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "        transforms.Normalize(normalize_means, normalize_stds)  # Normalize with specified means and stds\n",
    "    ])\n",
    "\n",
    "def get_normal_transform(resize_size=(224, 224), normalize_means=(0.5, 0.5, 0.5), normalize_stds=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    Returns the transformations for the validation/test dataset (without data augmentation).\n",
    "\n",
    "    Args:\n",
    "        resize_size (tuple): The size to resize the image.\n",
    "        normalize_means (tuple): Means for normalization.\n",
    "        normalize_stds (tuple): Standard deviations for normalization.\n",
    "\n",
    "    Returns:\n",
    "        transforms.Compose: The composed transformations for the validation/test set.\n",
    "    \"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(resize_size),  # Resize to specified size\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "        transforms.Normalize(normalize_means, normalize_stds)  # Normalize with specified means and stds\n",
    "    ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37d748-9b19-40ee-9e42-2c681203548a",
   "metadata": {
    "id": "ac37d748-9b19-40ee-9e42-2c681203548a"
   },
   "source": [
    "# Model Development \n",
    "#### In this stage, I will use the resampled dataset to address size constraints and ensure efficient model development. The resampled dataset allows for faster computations while preserving the data's core characteristics, which is critical for iterative model development and evaluation.\n",
    "\n",
    "#### I will develop three multi-input neural network models with slight variations in the image processing component. Each of these models will accept two inputs â€” image data and metadata â€” which will be processed independently before being combined for final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FcSXckhi0H8i",
   "metadata": {
    "id": "FcSXckhi0H8i"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # this will deetct "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da66b8-21d6-46ae-a01f-c678a616244a",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ba58d-5d31-4c26-acc1-3cfbe00104c2",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283e95c-0b5f-4978-b353-65e3aa7c54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFeatureCNN2(nn.Module):\n",
    "    def __init__(self, feature_input_size, input_image_size=(128, 128)):\n",
    "        super(CustomImageFeatureCNN2, self).__init__()\n",
    "\n",
    "        # Image CNN with Batch Normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization after conv1\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization after conv2\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization after conv3\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 2x2 Max pooling\n",
    "\n",
    "        # Dynamically calculate the flattened size of the feature map\n",
    "        self.flattened_size = self._get_flattened_size(input_image_size)\n",
    "\n",
    "        # Fully connected layer after the CNN layers\n",
    "        self.fc_image = nn.Linear(self.flattened_size, 512)\n",
    "\n",
    "        # Fully connected layer for metadata (feature data)\n",
    "        self.fc_metadata = nn.Linear(feature_input_size, 128)\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% dropout\n",
    "\n",
    "        # Final fully connected layer for binary classification (combined image + feature input)\n",
    "        self.fc_combined = nn.Linear(512 + 128, 1)  # Change 2 to 1 for binary classification\n",
    "\n",
    "    def _get_flattened_size(self, input_image_size):\n",
    "        # Forward pass a dummy image to get the size of the flattened features\n",
    "        dummy_image = torch.zeros(1, 3, *input_image_size)  # Batch size of 1, 3 channels (RGB), and input size\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(dummy_image))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        return x.view(-1).shape[0]  # Flatten and return the size\n",
    "\n",
    "    def forward(self, image, metadata):\n",
    "        # Forward pass for the image through the CNN\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(image))))  # Conv layer 1 with ReLU, BatchNorm, MaxPool\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Conv layer 2 with ReLU, BatchNorm, MaxPool\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Conv layer 3 with ReLU, BatchNorm, MaxPool\n",
    "\n",
    "        # Flatten the feature map to feed into fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature maps into a 1D vector\n",
    "        image_features = F.relu(self.fc_image(x))\n",
    "\n",
    "        # Process metadata (feature data)\n",
    "        metadata_features = F.relu(self.fc_metadata(metadata))\n",
    "\n",
    "        # Ensure the batch sizes are consistent\n",
    "        assert image_features.shape[0] == metadata_features.shape[0], \\\n",
    "            f\"Batch sizes do not match! Image batch size: {image_features.shape[0]}, Metadata batch size: {metadata_features.shape[0]}\"\n",
    "\n",
    "        # Concatenate image features and metadata features\n",
    "        combined_features = torch.cat((image_features, metadata_features), dim=1)\n",
    "\n",
    "        # Dropout and final classification layer\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        output = self.fc_combined(combined_features)\n",
    "\n",
    "        # If you're using BCELoss, uncomment the next line to apply sigmoid\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c3b5d-6e41-479c-b559-cc7b729e4c77",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed3e2d-3f05-4d1e-ab6a-8053567024ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFeatureResNet(nn.Module):\n",
    "    def __init__(self, feature_input_size, pretrained=True):\n",
    "        super(CustomImageFeatureResNet, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet model for image feature extraction (ResNet18 in this case)\n",
    "        resnet = models.resnet18(pretrained=pretrained)  # Change to resnet50, resnet101 as needed\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "        # The output of ResNet18's last conv layer is 512-dimensional (for ResNet50, it would be 2048)\n",
    "        self.fc_image = nn.Linear(resnet.fc.in_features, 512)  # Adjust if using ResNet50\n",
    "\n",
    "        # Fully connected layer for metadata (feature data)\n",
    "        self.fc_metadata = nn.Linear(feature_input_size, 128)\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% dropout\n",
    "\n",
    "        # Final fully connected layer for binary classification (combined image + feature input)\n",
    "        self.fc_combined = nn.Linear(512 + 128, 1)  # For binary classification\n",
    "\n",
    "    def forward(self, image, metadata):\n",
    "        # Forward pass for the image through the ResNet (without the final classification layer)\n",
    "        x = self.resnet(image)  # ResNet feature extraction\n",
    "        x = x.view(x.size(0), -1)  # Flatten the ResNet output\n",
    "        image_features = F.relu(self.fc_image(x))\n",
    "\n",
    "        # Process metadata (feature data)\n",
    "        metadata_features = F.relu(self.fc_metadata(metadata))\n",
    "\n",
    "        # Ensure the batch sizes are consistent\n",
    "        assert image_features.shape[0] == metadata_features.shape[0], \\\n",
    "            f\"Batch sizes do not match! Image batch size: {image_features.shape[0]}, Metadata batch size: {metadata_features.shape[0]}\"\n",
    "\n",
    "        # Concatenate image features and metadata features\n",
    "        combined_features = torch.cat((image_features, metadata_features), dim=1)\n",
    "\n",
    "        # Dropout and final classification layer\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        output = self.fc_combined(combined_features)\n",
    "\n",
    "        # If you're using BCELoss, uncomment the next line to apply sigmoid\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc0542-1092-45ae-8945-92c280e96e2f",
   "metadata": {},
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84b6d1-272a-4a02-b252-b35e3bee4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFeatureEfficientNet(nn.Module):\n",
    "    def __init__(self, feature_input_size, pretrained=True):\n",
    "        super(CustomImageFeatureEfficientNet, self).__init__()\n",
    "\n",
    "        # Load a pretrained EfficientNet model for image feature extraction (EfficientNet-B0 in this case)\n",
    "        efficientnet = models.efficientnet_b0(pretrained=pretrained)  # You can change this to another EfficientNet version like B1 or B7\n",
    "        self.efficientnet = nn.Sequential(*list(efficientnet.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "        # The output of EfficientNet-B0's last conv layer is 1280-dimensional\n",
    "        self.fc_image = nn.Linear(1280, 512)  # Reduce dimension to match your custom architecture\n",
    "\n",
    "        # Fully connected layer for metadata (feature data)\n",
    "        self.fc_metadata = nn.Linear(feature_input_size, 128)\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% dropout\n",
    "\n",
    "        # Final fully connected layer for binary classification (combined image + feature input)\n",
    "        self.fc_combined = nn.Linear(512 + 128, 1)  # For binary classification\n",
    "\n",
    "    def forward(self, image, metadata):\n",
    "        # Forward pass for the image through EfficientNet (without the final classification layer)\n",
    "        x = self.efficientnet(image)  # EfficientNet feature extraction\n",
    "        x = x.view(x.size(0), -1)  # Flatten the EfficientNet output\n",
    "        image_features = F.relu(self.fc_image(x))\n",
    "\n",
    "        # Process metadata (feature data)\n",
    "        metadata_features = F.relu(self.fc_metadata(metadata))\n",
    "\n",
    "        # Ensure the batch sizes are consistent\n",
    "        assert image_features.shape[0] == metadata_features.shape[0], \\\n",
    "            f\"Batch sizes do not match! Image batch size: {image_features.shape[0]}, Metadata batch size: {metadata_features.shape[0]}\"\n",
    "\n",
    "        # Concatenate image features and metadata features\n",
    "        combined_features = torch.cat((image_features, metadata_features), dim=1)\n",
    "\n",
    "        # Dropout and final classification layer\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        output = self.fc_combined(combined_features)\n",
    "\n",
    "        # If you're using BCELoss, uncomment the next line to apply sigmoid\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d529c-ab86-4f02-8395-011e74b64187",
   "metadata": {
    "id": "ad7d529c-ab86-4f02-8395-011e74b64187"
   },
   "source": [
    "### Model Training\n",
    "#### This cell contains the score function as well as the training and validation loop. The score function calculates the partial AUC-above-TPR, a key evaluation metric that focuses on the model's performance in high true positive rate regions. This is critical for ensuring that malignant lesions are correctly classified.\n",
    "\n",
    "#### During the model training process, I implemented early stopping and model checkpointing to enhance performance and prevent overfitting. At each epoch, the model's validation loss is tracked, and if it achieves the lowest validation loss observed so far, the model is saved as the best model. This best-performing version will be used for later deployment, ensuring that only the most optimal and generalizable model is selected for real-world use. By doing so, I can ensure that the final deployed model achieves a balance between bias and variance while maintaining strong predictive performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8bcf5-d08f-4cc1-b77d-e005a07bc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute partial AUC-above-TPR\n",
    "def score(solution: np.array, submission: np.array, min_tpr: float = 0.80) -> float:\n",
    "    \"\"\"\n",
    "    Compute the partial AUC by focusing on a specific range of true positive rates (TPR).\n",
    "    \n",
    "    Args:\n",
    "        solution (np.array): Ground truth binary labels.\n",
    "        submission (np.array): Model predictions.\n",
    "        min_tpr (float): Minimum true positive rate to calculate partial AUC.\n",
    "    \n",
    "    Returns:\n",
    "        float: The calculated partial AUC.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the min_tpr is not within a valid range.\n",
    "    \"\"\"\n",
    "    # Rescale the target to handle sklearn limitations and flip the predictions\n",
    "    v_gt = abs(solution - 1)\n",
    "    v_pred = -1.0 * submission\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "    # Compute ROC curve using sklearn\n",
    "    fpr, tpr, _ = roc_curve(v_gt, v_pred)\n",
    "    if max_fpr is None or max_fpr == 1:\n",
    "        return auc(fpr, tpr)\n",
    "    if max_fpr <= 0 or max_fpr > 1:\n",
    "        raise ValueError(f\"Expected min_tpr in range [0, 1), got: {min_tpr}\")\n",
    "    \n",
    "    # Interpolate for partial AUC\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    partial_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Au_KwtXzxvS9",
   "metadata": {
    "id": "Au_KwtXzxvS9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training and validation loop function\n",
    "def train_and_validate(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    val_dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    "    best_model_path: str,\n",
    "    early_stopping_patience: int = 5,\n",
    "    min_tpr: float = 0.80\n",
    "    \n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Train and validate a PyTorch model with early stopping, AUROC, partial AUC, and error handling.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to be trained and validated.\n",
    "        train_dataloader (torch.utils.data.DataLoader): Dataloader for training data.\n",
    "        val_dataloader (torch.utils.data.DataLoader): Dataloader for validation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer to update the model.\n",
    "        epochs (int): Number of training epochs.\n",
    "        device (torch.device): The device (CPU or GPU) to use.\n",
    "        early_stopping_patience (int): Early stopping patience.\n",
    "        min_tpr (float): The minimum true positive rate for calculating partial AUC.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Start the training and validation loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        all_train_labels = []\n",
    "        all_train_probs = []\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch + 1}')\n",
    "\n",
    "        try:\n",
    "            # Loop through the training batches\n",
    "            for i, (image, metadata, labels) in enumerate(progress_bar):\n",
    "                image, metadata, labels = image.to(device), metadata.to(device), labels.float().to(device)\n",
    "                labels = labels.unsqueeze(1)  # Adjust labels to have the right shape for binary classification\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                probs = model(image, metadata)\n",
    "\n",
    "                if probs.shape != labels.shape:\n",
    "                    raise ValueError(f\"Shape mismatch: Predictions shape {probs.shape} does not match labels shape {labels.shape}\")\n",
    "\n",
    "                # Calculate loss and backpropagate\n",
    "                loss = criterion(probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update running loss\n",
    "                running_train_loss += loss.item()\n",
    "\n",
    "                # Store labels and predictions for accuracy calculations\n",
    "                all_train_labels.extend(labels.cpu().detach().numpy())\n",
    "                all_train_probs.extend(probs.cpu().detach().numpy())\n",
    "\n",
    "                # Calculate binary predictions for training accuracy\n",
    "                predicted_train = (probs >= 0.5).float()\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix(train_loss=running_train_loss / (i + 1))\n",
    "\n",
    "            # Calculate training accuracy and loss\n",
    "            train_accuracy = 100 * correct_train / total_train\n",
    "            train_losses.append(running_train_loss / len(train_dataloader))\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error during training loop: {ve}\")\n",
    "            break\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        progress_bar = tqdm(val_dataloader, desc=f'Validating Epoch {epoch + 1}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Loop through the validation batches\n",
    "                for i, (images, metadata, labels) in enumerate(progress_bar):\n",
    "                    images, metadata, labels = images.to(device), metadata.to(device), labels.float().to(device)\n",
    "                    labels = labels.unsqueeze(1)\n",
    "\n",
    "                    probs = model(images, metadata)\n",
    "\n",
    "                    loss = criterion(probs, labels)\n",
    "                    running_val_loss += loss.item()\n",
    "\n",
    "                    all_labels.extend(labels.cpu().detach().numpy())\n",
    "                    all_probs.extend(probs.cpu().detach().numpy())\n",
    "\n",
    "                    # Calculate binary predictions for validation accuracy\n",
    "                    predicted = (probs >= 0.5).float()\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    progress_bar.set_postfix(val_loss=running_val_loss / (i + 1))\n",
    "\n",
    "                val_accuracy = 100 * correct / total\n",
    "                val_loss = running_val_loss / len(val_dataloader)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "                # Calculate AUROC\n",
    "                try:\n",
    "                    valid_auroc = roc_auc_score(all_labels, all_probs)\n",
    "                except ValueError as ve:\n",
    "                    print(f\"AUROC Calculation Error: {ve}\")\n",
    "                    valid_auroc = 0.0\n",
    "\n",
    "                # Calculate partial AUC-above-TPR\n",
    "                try:\n",
    "                    partial_auroc = score(np.array(all_labels), np.array(all_probs), min_tpr=min_tpr)\n",
    "                except ValueError as ve:\n",
    "                    print(f\"Partial AUC Calculation Error: {ve}\")\n",
    "                    partial_auroc = 0.0\n",
    "\n",
    "                print(f'Epoch [{epoch}/{epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "                      f'Val Accuracy: {val_accuracy:.2f}%, Val AUROC: {valid_auroc:.4f}, Partial AUROC: {partial_auroc:.4f}')    \n",
    "\n",
    "                # Early stopping based on validation loss\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_epoch = epoch + 1\n",
    "                    early_stopping_counter = 0\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "                if early_stopping_counter >= early_stopping_patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during validation loop: {e}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Best Epoch: {best_epoch}, Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print('Training Complete')\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Generate classification report\n",
    "    try:\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(all_labels, (np.array(all_probs) >= 0.5).astype(int), target_names=['Class 0', 'Class 1']))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating classification report: {e}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955269b4-4b36-4549-b8ca-ca8e967392c8",
   "metadata": {},
   "source": [
    "### Ready DataLoader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ninBag9U7qXn",
   "metadata": {
    "id": "ninBag9U7qXn"
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "CNN_train_dataset = MultiInputDataset(hdf5_file='../data/raw/train_images.hdf5', csv_file='../data/processed/processed-train-metadata1.csv', transform=get_train_transform(resize_size=(128,128)))\n",
    "CNN_val_dataset = MultiInputDataset(hdf5_file='../data/raw/validation_image.hdf5', csv_file='../data/processed/processed-validation-metadata1.csv', transform=get_normal_transform(resize_size=(128,128)))\n",
    "# Create a DataLoader\n",
    "CNN_train_dataloader = DataLoader(CNN_train_dataset,  batch_size=64, shuffle=True)\n",
    "CNN_val_dataloader = DataLoader(CNN_val_dataset,  batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2401030-3492-4e37-b394-a1e64426facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "resnet_train_dataset = MultiInputDataset(hdf5_file='../data/raw/train_images.hdf5', csv_file='../data/processed/processed-train-metadata1.csv', transform=get_train_transform(resize_size=(225,225)))\n",
    "resnet_val_dataset = MultiInputDataset(hdf5_file='../data/raw/validation_image.hdf5', csv_file='../data/processed/processed-validation-metadata1.csv', transform=get_normal_transform(resize_size=(225,225)))\n",
    "# Create a DataLoader\n",
    "resnet_train_dataloader = DataLoader(resnet_train_dataset,  batch_size=64, shuffle=True)\n",
    "resnet_val_dataloader = DataLoader(resnet_val_dataset,  batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74797d-effc-49b9-85f4-6ad40f318fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "effnet_train_dataset = MultiInputDataset(hdf5_file='../data/raw/train_images.hdf5', csv_file='../data/processed/processed-train-metadata1.csv', transform=get_train_transform(resize_size=(224,224)))\n",
    "effnet_val_dataset = MultiInputDataset(hdf5_file='../data/raw/validation_image.hdf5', csv_file='../data/processed/processed-validation-metadata1.csv', transform=get_normal_transform(resize_size=(224,224)))\n",
    "# Create a DataLoader\n",
    "effnet_train_dataloader = DataLoader(effnet_train_dataset,  batch_size=64, shuffle=True)\n",
    "effnet_val_dataloader = DataLoader(effnet_val_dataset,  batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73db71b-9b00-4cd0-9387-91d80306b325",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996d248-ae5f-452d-86ab-e3b46bd9daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CustomImageFeatureCNN2(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model1.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "best_model_path = \"best_model1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500f51-f3cc-48b5-8fdd-0f9be0679927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(model1,CNN_train_dataloader, CNN_val_dataloader, criterion, optimizer, epochs, device ,best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa91e4c-31a6-4376-b216-40475a7d6f4b",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4c2d8-b384-45c6-a266-3b5865a15f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CustomImageFeatureCNN2(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model2.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.001)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "best_model_path = \"best_model2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc68a2b-ce56-45b1-862a-e4766825fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(model2,CNN_train_dataloader, CNN_val_dataloader, criterion, optimizer, epochs, device,best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54868dad-3468-47f8-9681-c3f2a19a2b28",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99cfe0-1d97-4701-b03c-61f2a05d9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = CustomImageFeatureCNN2(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model3.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "best_model_path = \"best_model3.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018961e5-a288-4a3c-a2ea-45ac812ae8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_train_dataloader = DataLoader(CNN_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "CNN_val_dataloader = DataLoader(CNN_val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d653a-49d5-4a88-9c90-bca51bd97f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(model3,CNN_train_dataloader, CNN_val_dataloader, criterion, optimizer, epochs, device, best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8f2d4-f629-4ce2-a985-79c35cf50c9a",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e67c6-4b10-4b7f-916b-947f4ad2fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = CustomImageFeatureResNet(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model4.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model4.parameters(), lr=0.001)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "best_model_path = \"best_model4.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7da93-358c-45ad-be33-938609994af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(model4,resnet_train_dataloader, resnet_val_dataloader, criterion, optimizer, epochs, device, best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e453f761-4ac1-491f-b48b-e89ad023f195",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e9f14-bc4c-4389-8621-9d5bda555245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = CustomImageFeatureResNet(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model5.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model5.parameters(), lr=0.001)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "best_model_path = \"best_model5.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f568-fe4f-404c-ac41-1d967ea9ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(model5,resnet_train_dataloader, resnet_val_dataloader, criterion, optimizer, epochs, device, best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bb180-0c12-4c3c-8057-54de08d5b6c8",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7af29-c91a-48b7-8614-983663ccc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = CustomImageFeatureResNet(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model6.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model6.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "best_model_path = \"best_model6.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56c766-6ae7-4022-9b5f-07f530329af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_train_dataloader = DataLoader(resnet_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "resnet_val_dataloader = DataLoader(resnet_val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1444354-8117-4fde-8334-62ce7f6061b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(model6,resnet_train_dataloader, resnet_val_dataloader, criterion, optimizer, epochs, device, best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40eb0e1-d23b-453e-b518-93e5e28b9279",
   "metadata": {},
   "source": [
    "## Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QB7_Ic9MT633",
   "metadata": {
    "id": "QB7_Ic9MT633"
   },
   "outputs": [],
   "source": [
    "model7 =  CustomImageFeatureEfficientNet(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model7.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model7.parameters(), lr= 1.1621608010269284e-05)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "best_model_path = \"best_model7.pth\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c0d0c-c1ba-4c10-a8cc-185bd2cc8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet_train_dataloader = DataLoader(effnet_train_dataset,  batch_size=batch_size, shuffle=True)\n",
    "effnet_val_dataloader = DataLoader(effnet_val_dataset,  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VP0bFWDkvOye",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "editable": true,
    "id": "VP0bFWDkvOye",
    "outputId": "716c1538-333d-419a-9846-a653404f5333",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_and_validate(model7,effnet_train_dataloader, effnet_val_dataloader, criterion, optimizer, epochs, device, best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OFMQCQpldSHJ",
   "metadata": {
    "id": "OFMQCQpldSHJ"
   },
   "source": [
    "## Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RprU4gYXoC_4",
   "metadata": {
    "id": "RprU4gYXoC_4"
   },
   "outputs": [],
   "source": [
    "model8 = CustomImageFeatureEfficientNet(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model8.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model8.parameters(), lr=0.01)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "best_model_path = \"best_model8.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czjkFLKEo9DA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czjkFLKEo9DA",
    "outputId": "4b4608d5-62f3-4d0b-b3ad-23561bd6f020"
   },
   "outputs": [],
   "source": [
    "train_and_validate(model8,effnet_train_dataloader, effnet_val_dataloader, criterion, optimizer, epochs, device, best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644c143-cf66-47fd-a14f-48562bf4e6a0",
   "metadata": {},
   "source": [
    "## Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IRGVtC5lpP6M",
   "metadata": {
    "id": "IRGVtC5lpP6M"
   },
   "outputs": [],
   "source": [
    "model9 = CustomImageFeatureEfficientNet(feature_input_size=9)  # Assuming 9 features for metadata\n",
    "model9.to(device)\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model9.parameters(), lr=0.001)\n",
    "# Define the loss function with the class weights\n",
    "criterion = nn.BCELoss()  # Binary classification loss\n",
    "# Set the number of epochs\n",
    "epochs = 20\n",
    "batch_sizes = 16\n",
    "best_model_path = \"best_model9.path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "etUGo_AEpviL",
   "metadata": {
    "id": "etUGo_AEpviL"
   },
   "outputs": [],
   "source": [
    "train_and_validate(model9,effnet_train_dataloader, effnet_val_dataloader, criterion, optimizer, epochs, device, best_model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c599f-1263-4df6-8f89-7204a1571b7c",
   "metadata": {},
   "source": [
    "# Select Winning Model \n",
    "#### Based on the performance metrics of the 9 models, Model 7 has been selected as the winning model. This decision was made after evaluating each model's performance on key metrics such as accuracy, AUROC, partial AUC, loss, precision, and recall.\n",
    "\n",
    "#### The next step is to evaluate Model 7 on the test data, which contains unseen data that was not used during training or validation. This step is essential to assess the model's ability to generalize to new, real-world cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XcaJflPD3GOM",
   "metadata": {
    "id": "XcaJflPD3GOM"
   },
   "outputs": [],
   "source": [
    "effnet_test_dataset = MultiInputDataset(hdf5_file='../data/raw/test_image.hdf5', csv_file='../data/processed/processed-test-metadata1.csv', transform=get_normal_transform(resize_size=(224,224)))\n",
    "# Create a DataLoader\n",
    "effnet_test_dataloader = DataLoader(effnet_test_dataset,  batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df69515-360d-4f58-b8df-bd75d262d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = CustomImageFeatureEfficientNet(9)\n",
    "final_model_path = \"best_model7.pth\"\n",
    "final_model.load_state_dict(torch.load(final_model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "final_model.eval()\n",
    "all_labels, all_probs = [], []\n",
    "with torch.no_grad():\n",
    "    for images, metadata, labels in effnet_test_dataloader:\n",
    "        images, metadata, labels = images.to(device), metadata.to(device), labels.float().to(device).unsqueeze(1)\n",
    "        probs = final_model(images,metadata)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        predicted = (probs > 0.5).float()\n",
    "\n",
    "    partial_auroc=score(np.array(all_labels),np.array(all_probs))\n",
    "    print(f'The partial auroc of the final model on the test image is {partial_auroc}')\n",
    "    print(classification_report(all_labels, (np.array(all_probs) >= 0.5).astype(int), target_names=['Class 0', 'Class 1']))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680d01c-3b55-405f-affe-edabab11b5a6",
   "metadata": {},
   "source": [
    "#### As I expected, comparing the performance metrics on the test data versus the validation data for Model 7 reveals an improvement in the recall and F1-score for Class 1. This is a significant observation because it indicates that the model generalizes well to unseen data, which is crucial for real-world applications. Additionally, the partial AUC-above-TPR also shows improvement compared to the best epoch of Model 7 during validation. This suggests that the model performs better in capturing true positive malignant cases in regions of high true positive rates (TPR), which aligns with the primary goal of detecting malignant skin lesions effectively. These results demonstrate that the model is not overfitting to the validation set and is capable of making accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed747419-1606-4a5b-bc02-ee13fc1e318a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
